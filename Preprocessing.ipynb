{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import pickle\n",
    "import codecs\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"train.from\",'r') as fr:\n",
    "    frlist = fr.read().splitlines()  # every line in the input file is an ordered element in the list\n",
    "with open(\"train.to\",'r') as to:\n",
    "    tolist = to.read().splitlines()  # every line in the input file is an ordered element in the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3046050\n",
      "3046051\n"
     ]
    }
   ],
   "source": [
    "print(len(tolist))\n",
    "print(len(frlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'asd ill cant some ! hello non fshkd so was am https wwwhowccom dasd wont'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def unicode_to_ascii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def normalize_string(s):\n",
    "    s = unicode_to_ascii(s.lower().strip())\n",
    "    s = ' '.join(s.split())\n",
    "    s = s.replace(\"'\",\"\")\n",
    "    s = s.replace(\"newlinechar\",\"\")\n",
    "    s = s.replace(\".\",\"\")\n",
    "    s = s.replace(\"'\",\"\")\n",
    "    s = s.replace(\".\",\"\")\n",
    "    s = s.replace(\"n t \",\"nt \")\n",
    "    s = s.replace(\"i m \",\"im \")\n",
    "    s = s.replace(\"t s \",\"ts \")\n",
    "    s = s.replace(\" s \",\"s \")\n",
    "    s = s.replace(\" re \",\" are \")\n",
    "    s = s.replace(\"i ve \",\"ive \")\n",
    "    s = s.replace(\" d \",\"d \")\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s\n",
    "\n",
    "normalize_string(\" asd i'll can't some' #$%$!@%$% \\n \\t \\\\\\hello \\\\ non-fshkd, so was/am https://www.howc.com @dasd won't\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 100300\n",
      "10000 / 100300\n",
      "20000 / 100300\n",
      "30000 / 100300\n",
      "40000 / 100300\n",
      "50000 / 100300\n",
      "60000 / 100300\n",
      "70000 / 100300\n",
      "80000 / 100300\n",
      "90000 / 100300\n",
      "100000 / 100300\n"
     ]
    }
   ],
   "source": [
    "# At some point after 100300 the to and from stop matching up, \n",
    "# as you can see the lengths are different, not sure what went wrong\n",
    "\n",
    "last_line = 100300\n",
    "max_length = 25\n",
    "min_length = 1\n",
    "min_word_count = 5\n",
    "pickle_name = \"maxlen20_minlen2_minwrd5.p\"\n",
    "textpath = '1-25.txt'\n",
    "\n",
    "a = [\"?\", \"why\", \"what\", \"when\", \"where\", \"who\", \"whats\", \"whose\", \"how\", \n",
    "     \"someone\", \"sad\", \"happy\", \"depressed\", \"shy\", \"embarrass\", \"feeling\"]\n",
    "b = [\"love\", \"ok\", \"caring\", \"better\", \"universe\", \"home\", \"friend\", \"help\", \n",
    "     \"world\", \"hope\", \"help\", \"buddy\", \"together\",\"feeling\"]\n",
    "\n",
    "for i in range(last_line):\n",
    "    with open(textpath,'a') as o:\n",
    "        input_sentence = normalize_string(frlist[i])\n",
    "        output_sentence = normalize_string(tolist[i])\n",
    "        #output_sentence = input_sentence.replace(\"science\",\"science hell yea !\")\n",
    "        \n",
    "        if  (any(x in input_sentence for x in a) or any(y in output_sentence for y in b)) and \\\n",
    "            len(input_sentence.split(\" \")) < max_length and \\\n",
    "            len(input_sentence.split(\" \")) < max_length and \\\n",
    "            len(input_sentence.split(\" \")) > min_length and \\\n",
    "            len(output_sentence.split(\" \")) > min_length:\n",
    "                \n",
    "                \n",
    "            o.write('{}\\t{}\\n'.format(input_sentence, output_sentence))\n",
    "        \n",
    "    if i % 10000 == 0:\n",
    "        print(i,\"/\", last_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 / 304713\n",
      "2000 / 304713\n",
      "3000 / 304713\n",
      "4000 / 304713\n",
      "5000 / 304713\n",
      "6000 / 304713\n",
      "7000 / 304713\n",
      "8000 / 304713\n",
      "9000 / 304713\n",
      "10000 / 304713\n",
      "11000 / 304713\n",
      "12000 / 304713\n",
      "13000 / 304713\n",
      "14000 / 304713\n",
      "15000 / 304713\n",
      "16000 / 304713\n",
      "17000 / 304713\n",
      "18000 / 304713\n",
      "19000 / 304713\n",
      "20000 / 304713\n",
      "21000 / 304713\n",
      "22000 / 304713\n",
      "23000 / 304713\n",
      "24000 / 304713\n",
      "25000 / 304713\n",
      "26000 / 304713\n",
      "27000 / 304713\n",
      "28000 / 304713\n",
      "29000 / 304713\n",
      "30000 / 304713\n",
      "31000 / 304713\n",
      "32000 / 304713\n",
      "33000 / 304713\n",
      "34000 / 304713\n",
      "35000 / 304713\n",
      "36000 / 304713\n",
      "37000 / 304713\n",
      "38000 / 304713\n",
      "39000 / 304713\n",
      "40000 / 304713\n",
      "41000 / 304713\n",
      "42000 / 304713\n",
      "43000 / 304713\n",
      "44000 / 304713\n",
      "45000 / 304713\n",
      "46000 / 304713\n",
      "47000 / 304713\n",
      "48000 / 304713\n",
      "49000 / 304713\n",
      "50000 / 304713\n",
      "51000 / 304713\n",
      "52000 / 304713\n",
      "53000 / 304713\n",
      "54000 / 304713\n",
      "55000 / 304713\n",
      "56000 / 304713\n",
      "57000 / 304713\n",
      "58000 / 304713\n",
      "59000 / 304713\n",
      "60000 / 304713\n",
      "61000 / 304713\n",
      "62000 / 304713\n",
      "63000 / 304713\n",
      "64000 / 304713\n",
      "65000 / 304713\n",
      "66000 / 304713\n",
      "67000 / 304713\n",
      "68000 / 304713\n",
      "69000 / 304713\n",
      "70000 / 304713\n",
      "71000 / 304713\n",
      "72000 / 304713\n",
      "73000 / 304713\n",
      "74000 / 304713\n",
      "75000 / 304713\n",
      "76000 / 304713\n",
      "77000 / 304713\n",
      "78000 / 304713\n",
      "79000 / 304713\n",
      "80000 / 304713\n",
      "81000 / 304713\n",
      "82000 / 304713\n",
      "83000 / 304713\n",
      "84000 / 304713\n",
      "85000 / 304713\n",
      "86000 / 304713\n",
      "87000 / 304713\n",
      "88000 / 304713\n",
      "89000 / 304713\n",
      "90000 / 304713\n",
      "91000 / 304713\n",
      "92000 / 304713\n",
      "93000 / 304713\n",
      "94000 / 304713\n",
      "95000 / 304713\n",
      "96000 / 304713\n",
      "97000 / 304713\n",
      "98000 / 304713\n",
      "99000 / 304713\n",
      "100000 / 304713\n",
      "101000 / 304713\n",
      "102000 / 304713\n",
      "103000 / 304713\n",
      "104000 / 304713\n",
      "105000 / 304713\n",
      "106000 / 304713\n",
      "107000 / 304713\n",
      "108000 / 304713\n",
      "109000 / 304713\n",
      "110000 / 304713\n",
      "111000 / 304713\n",
      "112000 / 304713\n",
      "113000 / 304713\n",
      "114000 / 304713\n",
      "115000 / 304713\n",
      "116000 / 304713\n",
      "117000 / 304713\n",
      "118000 / 304713\n",
      "119000 / 304713\n",
      "120000 / 304713\n",
      "121000 / 304713\n",
      "122000 / 304713\n",
      "123000 / 304713\n",
      "124000 / 304713\n",
      "125000 / 304713\n",
      "126000 / 304713\n",
      "127000 / 304713\n",
      "128000 / 304713\n",
      "129000 / 304713\n",
      "130000 / 304713\n",
      "131000 / 304713\n",
      "132000 / 304713\n",
      "133000 / 304713\n",
      "134000 / 304713\n",
      "135000 / 304713\n",
      "136000 / 304713\n",
      "137000 / 304713\n",
      "138000 / 304713\n",
      "139000 / 304713\n",
      "140000 / 304713\n",
      "141000 / 304713\n",
      "142000 / 304713\n",
      "143000 / 304713\n",
      "144000 / 304713\n",
      "145000 / 304713\n",
      "146000 / 304713\n",
      "147000 / 304713\n",
      "148000 / 304713\n",
      "149000 / 304713\n",
      "150000 / 304713\n",
      "151000 / 304713\n",
      "152000 / 304713\n",
      "153000 / 304713\n",
      "154000 / 304713\n",
      "155000 / 304713\n",
      "156000 / 304713\n",
      "157000 / 304713\n",
      "158000 / 304713\n",
      "159000 / 304713\n",
      "160000 / 304713\n",
      "161000 / 304713\n",
      "162000 / 304713\n",
      "163000 / 304713\n",
      "164000 / 304713\n",
      "165000 / 304713\n",
      "166000 / 304713\n",
      "167000 / 304713\n",
      "168000 / 304713\n",
      "169000 / 304713\n",
      "170000 / 304713\n",
      "171000 / 304713\n",
      "172000 / 304713\n",
      "173000 / 304713\n",
      "174000 / 304713\n",
      "175000 / 304713\n",
      "176000 / 304713\n",
      "177000 / 304713\n",
      "178000 / 304713\n",
      "179000 / 304713\n",
      "180000 / 304713\n",
      "181000 / 304713\n",
      "182000 / 304713\n",
      "183000 / 304713\n",
      "184000 / 304713\n",
      "185000 / 304713\n",
      "186000 / 304713\n",
      "187000 / 304713\n",
      "188000 / 304713\n",
      "189000 / 304713\n",
      "190000 / 304713\n",
      "191000 / 304713\n",
      "192000 / 304713\n",
      "193000 / 304713\n",
      "194000 / 304713\n",
      "195000 / 304713\n",
      "196000 / 304713\n",
      "197000 / 304713\n",
      "198000 / 304713\n",
      "199000 / 304713\n",
      "200000 / 304713\n",
      "201000 / 304713\n",
      "202000 / 304713\n",
      "203000 / 304713\n",
      "204000 / 304713\n",
      "205000 / 304713\n",
      "206000 / 304713\n",
      "207000 / 304713\n",
      "208000 / 304713\n",
      "209000 / 304713\n",
      "210000 / 304713\n",
      "211000 / 304713\n",
      "212000 / 304713\n",
      "213000 / 304713\n",
      "214000 / 304713\n",
      "215000 / 304713\n",
      "216000 / 304713\n",
      "217000 / 304713\n",
      "218000 / 304713\n",
      "219000 / 304713\n",
      "220000 / 304713\n",
      "221000 / 304713\n",
      "222000 / 304713\n",
      "223000 / 304713\n",
      "224000 / 304713\n",
      "225000 / 304713\n",
      "226000 / 304713\n",
      "227000 / 304713\n",
      "228000 / 304713\n",
      "229000 / 304713\n",
      "230000 / 304713\n",
      "231000 / 304713\n",
      "232000 / 304713\n",
      "233000 / 304713\n",
      "234000 / 304713\n",
      "235000 / 304713\n",
      "236000 / 304713\n",
      "237000 / 304713\n",
      "238000 / 304713\n",
      "239000 / 304713\n",
      "240000 / 304713\n",
      "241000 / 304713\n",
      "242000 / 304713\n",
      "243000 / 304713\n",
      "244000 / 304713\n",
      "245000 / 304713\n",
      "246000 / 304713\n",
      "247000 / 304713\n",
      "248000 / 304713\n",
      "249000 / 304713\n",
      "250000 / 304713\n",
      "251000 / 304713\n",
      "252000 / 304713\n",
      "253000 / 304713\n",
      "254000 / 304713\n",
      "255000 / 304713\n",
      "256000 / 304713\n",
      "257000 / 304713\n",
      "258000 / 304713\n",
      "259000 / 304713\n",
      "260000 / 304713\n",
      "261000 / 304713\n",
      "262000 / 304713\n",
      "263000 / 304713\n",
      "264000 / 304713\n",
      "265000 / 304713\n",
      "266000 / 304713\n",
      "267000 / 304713\n",
      "268000 / 304713\n",
      "269000 / 304713\n",
      "270000 / 304713\n",
      "271000 / 304713\n",
      "272000 / 304713\n",
      "273000 / 304713\n",
      "274000 / 304713\n",
      "275000 / 304713\n",
      "276000 / 304713\n",
      "277000 / 304713\n",
      "278000 / 304713\n",
      "279000 / 304713\n",
      "280000 / 304713\n",
      "281000 / 304713\n",
      "282000 / 304713\n",
      "283000 / 304713\n",
      "284000 / 304713\n",
      "285000 / 304713\n",
      "286000 / 304713\n",
      "287000 / 304713\n",
      "288000 / 304713\n",
      "289000 / 304713\n",
      "290000 / 304713\n",
      "291000 / 304713\n",
      "292000 / 304713\n",
      "293000 / 304713\n",
      "294000 / 304713\n",
      "295000 / 304713\n",
      "296000 / 304713\n",
      "297000 / 304713\n",
      "298000 / 304713\n",
      "299000 / 304713\n",
      "300000 / 304713\n",
      "301000 / 304713\n",
      "302000 / 304713\n",
      "303000 / 304713\n",
      "304000 / 304713\n"
     ]
    }
   ],
   "source": [
    "utterance_dict = pickle.load( open( \"utterance_dict.p\", \"rb\" ) )\n",
    "\n",
    "i = 0\n",
    "with open(textpath,'a') as o:\n",
    "\n",
    "    for line_code, utterance in utterance_dict.items():\n",
    "\n",
    "        index = int(re.findall('\\d+', line_code)[0])\n",
    "        \n",
    "        utterance = normalize_string(utterance)\n",
    "        \n",
    "        \n",
    "\n",
    "        if \"L\"+str(index-1) in utterance_dict:\n",
    "            if len(utterance.split(' ')) < max_length and len(utterance.split(' ')) > min_length:\n",
    "                input_sentence = normalize_string(utterance_dict[\"L\"+str(index-1)])\n",
    "                \n",
    "                #if any(x in input_sentence for x in a) or any(y in utterance for y in b):\n",
    "                if len(input_sentence.split(' ')) < max_length and len(input_sentence.split(' ')) > min_length:\n",
    "                    #print(\"yes\")\n",
    "                    o.write('{}\\t{}\\n'.format(input_sentence, utterance))\n",
    "\n",
    "        if \"L\"+str(index+1) in utterance_dict:\n",
    "            if len(utterance_dict[\"L\"+str(index+1)].split(' ')) < max_length and len(utterance_dict[\"L\"+str(index+1)].split(' ')) > min_length:\n",
    "                output_sentence = normalize_string(utterance_dict[\"L\"+str(index+1)])\n",
    "                #if any(x in utterance for x in a) or any(y in output_sentence for y in b):\n",
    "                if len(output_sentence.split(' ')) < max_length and len(output_sentence.split(' ')) > min_length:\n",
    "                    #print(\"yes\")\n",
    "                    o.write('{}\\t{}\\n'.format(utterance,  output_sentence))\n",
    "\n",
    "        i+=1\n",
    "        \n",
    "        if i % 1000 == 0:\n",
    "            print(i,\"/\",len(utterance_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trimming...\n",
      "keep_words 11501 / 41322 = 0.2783\n",
      "building embedding from glove...\n",
      "embedding done...\n",
      "maxlen10_minlen2_minwrd5.p done\n"
     ]
    }
   ],
   "source": [
    "class Lang:\n",
    "    \n",
    "    def __init__(self, name):\n",
    "        \n",
    "        '''\n",
    "        Store the string token to index token\n",
    "        mapping in the word2index and index2word\n",
    "        dictionaries. \n",
    "        '''\n",
    "        \n",
    "        self.name = name\n",
    "        self.trimmed = False # gets changed to True first time Lang.trim(min_count) is called\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n",
    "        self.n_words = len(self.index2word) # Count default tokens\n",
    "        self.num_nonwordtokens = len(self.index2word)\n",
    "        self.PAD_token = 0\n",
    "        self.SOS_token = 1\n",
    "        self.EOS_token = 2\n",
    "        self.UNK_token = 3\n",
    "\n",
    "    def index_sentence(self, sentence):\n",
    "        '''\n",
    "        Absorbs a sentence string into the token dictionary\n",
    "        one word at a time using the index_word function\n",
    "        increments the word count dictionary as well\n",
    "        '''\n",
    "        for word in sentence.split(' '):\n",
    "            self.index_word(word)\n",
    "\n",
    "    def index_word(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "\n",
    "    \n",
    "    def trim(self, min_count):\n",
    "        '''\n",
    "        Removes words from our 3 dictionaries that\n",
    "        are below a certain count threshold (min_count)\n",
    "        '''\n",
    "        if self.trimmed: return\n",
    "        self.trimmed = True\n",
    "        \n",
    "        keep_words = []\n",
    "        \n",
    "        for k, v in self.word2count.items():\n",
    "            if v >= min_count:\n",
    "                keep_words.append(k)\n",
    "\n",
    "        print('keep_words %s / %s = %.4f' % (\n",
    "            len(keep_words), len(self.word2index), len(keep_words) / len(self.word2index)\n",
    "        ))\n",
    "\n",
    "        # Reinitialize dictionaries\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n",
    "        self.n_words = len(self.index2word) # Count default tokens\n",
    "        self.num_nonwordtokens = len(self.index2word)\n",
    "        self.PAD_token = 0\n",
    "        self.SOS_token = 1\n",
    "        self.EOS_token = 2\n",
    "        self.UNK_token = 3\n",
    "\n",
    "        for word in keep_words:\n",
    "            self.index_word(word)\n",
    "            \n",
    "lang = Lang(\"chat\")\n",
    "\n",
    "\n",
    "lines = open(textpath).read().strip().split('\\n')\n",
    "\n",
    "final_pairs = []\n",
    "\n",
    "i = 0\n",
    "\n",
    "for l in lines:\n",
    "\n",
    "    pair = l.split('\\t')\n",
    "    \n",
    "    if len(pair) != 2:\n",
    "        print(pair)\n",
    "\n",
    "    filtered_pair = []\n",
    "\n",
    "    for sentence in pair:\n",
    "\n",
    "        lang.index_sentence(sentence)\n",
    "        filtered_pair.append(sentence)\n",
    "\n",
    "    final_pairs.append(filtered_pair)\n",
    "    \n",
    "print(\"trimming...\")    \n",
    "lang.trim(min_word_count)\n",
    "\n",
    "embed_dim = embedding.shape[1]\n",
    "\n",
    "print(\"building embedding from glove...\")\n",
    "\n",
    "embed = np.zeros((len(lang.index2word), embed_dim)).astype(np.float32)\n",
    "\n",
    "for i in range(lang.num_nonwordtokens):\n",
    "    \n",
    "    embed[i,:] = np.random.uniform(-.1,.1,embed_dim).astype(np.float32)\n",
    "    \n",
    "for i in range(lang.num_nonwordtokens,len(lang.index2word)):\n",
    "    \n",
    "    if lang.index2word[i] in word2index:\n",
    "        embed[i,:] = embedding[word2index[lang.index2word[i]],:]\n",
    "    else:\n",
    "        embed[i,:] = np.random.uniform(-.1,.1,embed_dim).astype(np.float32)\n",
    "\n",
    "print(\"embedding shape\", embed.shape)\n",
    "datatest = (embed,lang.word2index,lang.index2word,final_pairs)\n",
    "pickle.dump(datatest, open( pickle_name, \"wb\" ))\n",
    "print(pickle_name + \" done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datatest = (embed,lang.word2index,lang.index2word,final_pairs)\n",
    "pickle.dump(datatest, open( pickle_name, \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.586036"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(embed[lang.word2index[\"i\"]],embed[lang.word2index[\"science\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35.502968"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(embed[lang.word2index[\"i\"]],embed[lang.word2index[\"me\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datatest = (embed,lang.word2index,lang.index2word,final_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(datatest, open( \"small_4_10.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13947, 50)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "147690"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
